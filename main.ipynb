{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from dataset import SemanticSegmentationDataset\n",
    "from models.unet import UNet\n",
    "\n",
    "# define hyperparameters\n",
    "hyperparameters = {}\n",
    "hyperparameters['lr'] = 1e-3\n",
    "hyperparameters['batch_size'] = 1\n",
    "hyperparameters['epochs'] = 100\n",
    "hyperparameters['loss'] = 'CE'\n",
    "# hyperparameters['loss'] = 'Focal'\n",
    "# hyperparameters['loss'] = 'Dice'\n",
    "# hyperparameters['loss'] = 'Combo'\n",
    "\n",
    "\n",
    "\n",
    "# potential categories, choose one set\n",
    "categories = {}\n",
    "categories[\"Class Rarely Or Never Used\"] = [\"Other Material\", \"DisturbeView\", \"VesselLinked\", \"SolidLargChunk\", \"PropertiesMaterialInFront\", \"Other Material\"]\n",
    "categories[\"Vessel Type Class\"] = [\"Vessel\", \"Syringe\", \"Pippete\", \"Tube\", \"IVBag\", \"DripChamber\", \"IVBottle\", \"Beaker\", \"RoundFlask\", \"Cylinder\", \"SeparatoryFunnel\", \"Funnel\", \"Burete\", \"ChromatographyColumn\", \"Condenser\", \"Bottle\", \"Jar\", \"Connector\", \"Flask\", \"Cup\", \"Bowl\", \"Erlenmeyer\", \"Vial\", \"Dish\", \"HeatingVessel\", \"Tube\"]\n",
    "categories[\"Vessel Properties Class\"] = [\"Transparent\", \"SemiTrans\", \"Opaque\", \"DisturbeView\", \"VesselInsideVessel\"]\n",
    "categories['Vessel Part Class'] = [\"Cork\", \"Label\", \"Part\", \"Spike\", \"Valve\", \"MagneticStirer\", \"Thermometer\", \"Spatula\", \"Holder\", \"Filter\", \"PipeTubeStraw\"]\n",
    "categories[\"Material Property Class\"] = [\"MaterialOnSurface\", \"MaterialScattered\", \"PropertiesMaterialInsideImmersed\", \"PropertiesMaterialInFront\"]\n",
    "categories[\"Material Type Class\"] =  [\"Liquid\", \"Foam\", \"Suspension\", \"Solid\", \"Powder\", \"Urine\", \"Blood\", \"Gel\", \"Granular\", \"SolidLargChunk\", \"Vapor\", \"Other Material\", \"Filled\"]\n",
    "categories_set = \"Vessel Part Class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set\n",
    "train_data = SemanticSegmentationDataset('train', categories=categories[categories_set])\n",
    "test_data = SemanticSegmentationDataset('test', categories=categories[categories_set])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=hyperparameters['batch_size'])\n",
    "test_loader = DataLoader(test_data, batch_size=hyperparameters['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Loss functions </h2>\n",
    "<p> Going to run a few experiments with loss functions </p>\n",
    "<p> 1) Simple cross entropy loss </p>\n",
    "<p> 2) Class balanced focal loss </p>\n",
    "<p> 3) Dice loss </p>\n",
    "<p> 4) Combination of focal loss and dice loss </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model and opt\n",
    "# out channels == num classes\n",
    "model = UNet(in_channels=3, out_channels=len(categories[categories_set]))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparameters['lr'])\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one full pass of the shuffled data set with weight updates\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "\n",
    "    for idx, (image, semantic_maps) in enumerate(train_loader):\n",
    "        # TODO: doublecheck the handling of base and label samples for correct loss calculation\n",
    "        print(image.shape, semantic_maps.shape)\n",
    "        assert(0)\n",
    "\n",
    "        model_output = model(image)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model_output, semantic_maps)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss, total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one full pass through the eval set to assess performance\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (image, semantic_maps) in enumerate(test_loader):\n",
    "            # TODO: doublecheck the handling of base and label samples for correct loss calculation\n",
    "            model_output = model(image)\n",
    "\n",
    "            loss = criterion(model_output, semantic_maps)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss, total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jlbar\\anaconda3\\envs\\RL\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 572, 572]) torch.Size([1, 0])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# main loop\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(hyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m----> 3\u001b[0m     train_loss, avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     test_loss, avg_test_loss \u001b[38;5;241m=\u001b[39m evaluate()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m Training Loss: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m. Validation Loss: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e, avg_train_loss, avg_test_loss))\n",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (image, semantic_maps) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# TODO: doublecheck the handling of base and label samples for correct loss calculation\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape, semantic_maps\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     11\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m model(image)\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "for e in range(hyperparameters['epochs']):\n",
    "    train_loss, avg_train_loss = train()\n",
    "    test_loss, avg_test_loss = evaluate()\n",
    "    print(\"Epoch %d Training Loss: %.4f. Validation Loss: %.4f. \" % (e, avg_train_loss, avg_test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
